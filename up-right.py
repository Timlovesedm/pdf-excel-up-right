import streamlit as st
import pandas as pd
import pdfplumber
import io
import re
from collections import defaultdict

# ==========================================
# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ»PDFæŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ (Tool 1)
# ==========================================

def extract_tables_from_multiple_pdfs(pdf_files, keywords, start_page, end_page):
    all_rows = []
    if not keywords:
        st.error("â— ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", icon="ğŸš¨")
        return None
    for pdf_file in pdf_files:
        all_rows.append([f"ãƒ•ã‚¡ã‚¤ãƒ«å: {pdf_file.name}"])
        all_rows.append([])
        found_in_file = False
        try:
            with pdfplumber.open(pdf_file) as pdf:
                start_index = start_page - 1 if start_page else 0
                end_index = end_page if end_page else len(pdf.pages)
                target_pages = pdf.pages[start_index:end_index]
                for page in target_pages:
                    text = page.extract_text() or ""
                    if any(kw in text for kw in keywords):
                        found_in_file = True
                        tables = page.extract_tables()
                        for table_index, table in enumerate(tables):
                            if not table:
                                continue
                            all_rows.append([f"--- ãƒšãƒ¼ã‚¸ {page.page_number} / ãƒ†ãƒ¼ãƒ–ãƒ« {table_index + 1} ---"])
                            for row in table:
                                cleaned_row = ["" if item is None else str(item).replace("\n", " ") for item in row]
                                all_rows.append(cleaned_row)
                            all_rows.append([])
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{pdf_file.name}ã€å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", icon="ğŸ”¥")
            continue
        if not found_in_file:
            st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{pdf_file.name}ã€ã§ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¡¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", icon="âš ï¸")

    if not any(r for r in all_rows if r):
        return None
    return pd.DataFrame(all_rows)


# ==========================================
# ãƒ„ãƒ¼ãƒ«â‘¡ï¼šç¸¦æ–¹å‘çµ±åˆãƒ­ã‚¸ãƒƒã‚¯ (Vertical Integration)
# ==========================================

def tool2_extract_data_from_chunk_vertical(df_chunk):
    if df_chunk.empty:
        return None, []
    
    quarter_pat = re.compile(r"^\s*(20\d{2}Q[1-4])\s*$", re.IGNORECASE)
    from_date_pat = re.compile(r"\(è‡ª\s*(\d{4})å¹´(\d{1,2})æœˆ")
    date_pat = re.compile(r"\((\d{4})å¹´(\d{1,2})æœˆ") 
    year_pat = re.compile(r"^\s*20\d{2}(\d{2})?\s*$")

    year_cells = []
    for r in range(df_chunk.shape[0]):
        for c in range(df_chunk.shape[1]):
            cell_value = str(df_chunk.iat[r, c]).strip()
            year_header = None

            match_q = quarter_pat.search(cell_value)
            match1 = from_date_pat.search(cell_value)
            match2 = date_pat.search(cell_value)

            if match_q:
                year_header = match_q.group(1).upper()
            elif match1:
                year = match1.group(1)
                month = match1.group(2)
                year_header = f"{year}/{month}"
            elif match2:
                year = match2.group(1)
                month = match2.group(2)
                year_header = f"{year}/{month}"
            elif cell_value.isdigit() and year_pat.match(cell_value):
                year_header = cell_value

            if year_header:
                year_cells.append({"row": r, "col": c, "year_header": year_header})

    if not year_cells:
        return None, []

    year_cells.sort(key=lambda x: (x["row"], x["col"]))
    processed_years = set()
    initial_items = df_chunk[0].astype(str).str.strip().dropna()
    initial_items = initial_items[initial_items != ""]
    is_sonota = initial_items == "ãã®ä»–"
    if is_sonota.any():
        sonota_counts = initial_items.groupby(initial_items).cumcount()
        initial_items.loc[is_sonota] = "ãã®ä»–_temp_" + sonota_counts[is_sonota].astype(str)
    all_items_ordered = initial_items.drop_duplicates(keep="first").tolist()
    df_result = pd.DataFrame({"å…±é€šé …ç›®": all_items_ordered})

    for cell in year_cells:
        year_header = cell["year_header"]
        if year_header in processed_years:
            continue
        processed_years.add(year_header)
        val_col = cell["col"]
        temp_df = df_chunk.iloc[cell["row"] + 1 :, [0, val_col]].copy()
        temp_df.columns = ["å…±é€šé …ç›®", year_header]
        temp_df["å…±é€šé …ç›®"] = temp_df["å…±é€šé …ç›®"].astype(str).str.strip()
        temp_df = temp_df[temp_df["å…±é€šé …ç›®"] != ""].dropna(subset=["å…±é€šé …ç›®"])
        is_sonota = temp_df["å…±é€šé …ç›®"] == "ãã®ä»–"
        if is_sonota.any():
            sonota_counts = temp_df.groupby("å…±é€šé …ç›®").cumcount()
            temp_df.loc[is_sonota, "å…±é€šé …ç›®"] = "ãã®ä»–_temp_" + sonota_counts[is_sonota].astype(str)
        temp_df[year_header] = (
            pd.to_numeric(temp_df[year_header].astype(str).str.replace(",", ""), errors='coerce').fillna(0)
        )
        temp_df = temp_df.drop_duplicates(subset=["å…±é€šé …ç›®"], keep="first")
        df_result = pd.merge(df_result, temp_df, on="å…±é€šé …ç›®", how="left")

    return df_result, all_items_ordered


def process_files_and_tables_vertical(excel_file):
    try:
        xls = pd.ExcelFile(excel_file)
        sheet_name_to_read = "æŠ½å‡ºçµæœ" if "æŠ½å‡ºçµæœ" in xls.sheet_names else xls.sheet_names[0]
        df_full = pd.read_excel(xls, sheet_name=sheet_name_to_read, header=None)
    except Exception as e:
        st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return None

    df_full[0] = df_full[0].astype(str)
    file_indices = df_full[df_full[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:", na=False)].index.tolist()
    file_chunks = []
    if not file_indices:
        file_chunks.append(df_full)
    else:
        for i in range(len(file_indices)):
            start_idx = file_indices[i]
            end_idx = file_indices[i + 1] if i + 1 < len(file_indices) else len(df_full)
            file_chunks.append(df_full.iloc[start_idx:end_idx].reset_index(drop=True))

    grouped_tables = defaultdict(list)
    master_item_order = defaultdict(list)

    for file_chunk in file_chunks:
        page_indices = file_chunk[file_chunk[0].str.contains(r"--- ãƒšãƒ¼ã‚¸", na=False)].index.tolist()
        table_chunks = []
        last_idx = 0
        if not page_indices:
            clean_chunk = file_chunk[
                ~file_chunk[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:|---|^\s*$", na=False, regex=True)
            ].dropna(how="all")
            if not clean_chunk.empty:
                table_chunks.append(clean_chunk)
        else:
            for idx in page_indices:
                chunk = file_chunk.iloc[last_idx:idx]
                if not chunk.empty:
                    table_chunks.append(chunk)
                last_idx = idx
            final_chunk = file_chunk.iloc[last_idx:]
            if not final_chunk.empty:
                table_chunks.append(final_chunk)

        for i, table_chunk in enumerate(table_chunks):
            clean_table_chunk = table_chunk[
                ~table_chunk[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:|---", na=False, regex=True)
            ].dropna(how="all")
            if clean_table_chunk.empty:
                continue
            processed_df, item_order = tool2_extract_data_from_chunk_vertical(
                clean_table_chunk.reset_index(drop=True)
            )
            if processed_df is not None and not processed_df.empty:
                grouped_tables[i].append(processed_df)
                current_master_order = master_item_order[i]
                if not current_master_order:
                    master_item_order[i].extend(item_order)
                else:
                    last_known_index = -1
                    for item in item_order:
                        if item in current_master_order:
                            last_known_index = current_master_order.index(item)
                        else:
                            current_master_order.insert(last_known_index + 1, item)
                            last_known_index += 1

    final_summaries = []
    for table_index in sorted(grouped_tables.keys()):
        list_of_dfs = grouped_tables[table_index]
        ordered_items = master_item_order[table_index]
        if not list_of_dfs:
            continue
        result_df = pd.DataFrame({"å…±é€šé …ç›®": ordered_items})
        for df_to_merge in list_of_dfs:
            cols_to_drop = [
                col for col in df_to_merge.columns if col in result_df.columns and col != "å…±é€šé …ç›®"
            ]
            result_df = pd.merge(
                result_df, df_to_merge.drop(columns=cols_to_drop), on="å…±é€šé …ç›®", how="left"
            )
        result_df.fillna(0, inplace=True)
        year_cols = sorted(
            [col for col in result_df.columns if col != "å…±é€šé …ç›®"],
            key=lambda x: int(str(x).upper().replace('/', '').replace('Q', '0').ljust(6, '0'))
        )
        final_cols = ["å…±é€šé …ç›®"] + year_cols
        result_df = result_df[final_cols]
        for col in year_cols:
            result_df[col] = pd.to_numeric(result_df[col], errors='coerce').fillna(0).astype(int)
        result_df["å…±é€šé …ç›®"] = result_df["å…±é€šé …ç›®"].str.replace(r"_temp_\d+$", "", regex=True)
        final_summaries.append(result_df)
    return final_summaries


# ==========================================
# ãƒ„ãƒ¼ãƒ«â‘¡ï¼šæ¨ªæ–¹å‘çµ±åˆãƒ­ã‚¸ãƒƒã‚¯ (Horizontal) - å†æ§‹ç¯‰ç‰ˆ
# ==========================================

def split_into_horizontal_blocks(df):
    """
    ç©ºç™½åˆ—ã‚’åŒºåˆ‡ã‚Šã¨ã—ã¦ã€DataFrameã‚’è¤‡æ•°ã®ã€Œãƒ–ãƒ­ãƒƒã‚¯ï¼ˆè¡¨ï¼‰ã€ã«åˆ†å‰²ã™ã‚‹é–¢æ•°
    """
    blocks = []
    current_cols = []
    
    # åˆ—ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
    for col in df.columns:
        # åˆ—ãŒå…¨ã¦ç©ºï¼ˆNaNã¾ãŸã¯ç©ºæ–‡å­—ï¼‰ã‹ãƒã‚§ãƒƒã‚¯
        is_empty_col = df[col].astype(str).str.strip().replace("nan", "").eq("").all()
        
        if is_empty_col:
            if current_cols:
                blocks.append(df[current_cols].copy())
                current_cols = []
        else:
            current_cols.append(col)
            
    # æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ 
    if current_cols:
        blocks.append(df[current_cols].copy())
        
    return blocks

def extract_data_from_block(block_df):
    """
    1ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ {Year: {Item: Value}} ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹
    """
    # æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®šç¾©ï¼ˆç¸¦æ–¹å‘ã¨åŒã˜å¼·åŠ›ãªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    patterns = [
        re.compile(r"^\s*(20\d{2}Q[1-4])\s*$", re.IGNORECASE),
        re.compile(r"\(è‡ª\s*(\d{4})å¹´(\d{1,2})æœˆ"),
        re.compile(r"\((\d{4})å¹´(\d{1,2})æœˆ"),
        re.compile(r"20\d{2}") # ã‚·ãƒ³ãƒ—ãƒ«ãªå¹´å·
    ]
    
    year_header = None
    value_col_idx = None
    item_col_idx = None
    
    # 1. å¹´æ¬¡ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆæ•°å€¤åˆ—ï¼‰ã‚’æ¢ã™
    # ãƒ–ãƒ­ãƒƒã‚¯å†…ã®æœ€åˆã®æ•°è¡Œã‚’ã‚¹ã‚­ãƒ£ãƒ³
    for r in range(min(5, len(block_df))):
        for c in range(len(block_df.columns)):
            cell_val = str(block_df.iat[r, c]).strip()
            
            for pat in patterns:
                match = pat.search(cell_val)
                if match:
                    # å¹´å·ãŒè¦‹ã¤ã‹ã£ãŸ
                    found_year = match.group(0)
                    # æ­£è¦è¡¨ç¾ã®çµæœã‹ã‚‰ãã‚Œã„ãªå¹´å·æ–‡å­—åˆ—ã‚’ä½œã‚‹
                    nums = re.findall(r"20\d{2}", found_year)
                    if nums:
                        year_header = nums[0]
                    else:
                        year_header = found_year # Q1ãªã©ã‚’ãã®ã¾ã¾ä½¿ã†å ´åˆ
                    
                    value_col_idx = c
                    break
            if year_header: break
        if year_header: break
        
    if not year_header:
        return None, None
        
    # 2. é …ç›®åˆ—ã‚’æ¢ã™ï¼ˆæ•°å€¤åˆ—ã®å·¦å´ã«ã‚ã‚‹ã¨ä»®å®šï¼‰
    # åŸºæœ¬çš„ã«ä¸€ç•ªå·¦ã®åˆ—ã€ã‚‚ã—ãã¯æ•°å€¤åˆ—ã®1ã¤å·¦
    if value_col_idx > 0:
        item_col_idx = 0 # ãƒ–ãƒ­ãƒƒã‚¯ã®ä¸€ç•ªå·¦ã‚’é …ç›®åˆ—ã¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„
    else:
        return None, None # é …ç›®åˆ—ãŒãªã„
        
    # 3. ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    items = []
    values = []
    item_counter = defaultdict(int)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®æ¬¡ã‹ã‚‰ã‚¹ã‚­ãƒ£ãƒ³
    start_row = r + 1
    
    for i in range(start_row, len(block_df)):
        raw_item = str(block_df.iat[i, item_col_idx]).strip()
        raw_val = str(block_df.iat[i, value_col_idx]).strip()
        
        if raw_item and raw_item != "nan":
            # é‡è¤‡å‡¦ç†ï¼ˆA, B, ãã®ä»–ãªã©ï¼‰
            count = item_counter[raw_item]
            item_counter[raw_item] += 1
            
            if raw_item == "ãã®ä»–":
                item_name = f"{raw_item}_temp_{count}"
            elif count > 0:
                item_name = f"{raw_item}_{count}"
            else:
                item_name = raw_item
                
            # æ•°å€¤å‡¦ç†
            clean_val = raw_val.replace(",", "").replace("â–³", "-").replace("â–²", "-").strip()
            try:
                val = float(clean_val)
                if val.is_integer(): val = int(val)
            except:
                val = 0
            
            items.append(item_name)
            values.append(val)
            
    return year_header, pd.DataFrame({"å…±é€šé …ç›®": items, year_header: values})


def process_files_and_tables_horizontal(excel_file):
    """
    æ¨ªæ–¹å‘çµ±åˆã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    ãƒ–ãƒ­ãƒƒã‚¯åˆ†å‰² -> å„æŠ½å‡º -> ãƒãƒ¼ã‚¸ï¼ˆé †åºä¿æŒï¼‰
    """
    try:
        xls = pd.ExcelFile(excel_file)
        sheet_name_to_read = "æŠ½å‡ºçµæœ" if "æŠ½å‡ºçµæœ" in xls.sheet_names else xls.sheet_names[0]
        df_full = pd.read_excel(xls, sheet_name=sheet_name_to_read, header=None)
    except Exception as e:
        st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return None

    # 1. ãƒ•ã‚¡ã‚¤ãƒ«åè¡Œãªã©ã§å¤§ããªãƒãƒ£ãƒ³ã‚¯ã«åˆ†ã‘ã‚‹ï¼ˆTool 1ã®å‡ºåŠ›å½¢å¼ä¾å­˜ï¼‰
    # ãŸã ã—ã€æ¨ªæ–¹å‘ã®å ´åˆã¯1ã‚·ãƒ¼ãƒˆã«ã¾ã¨ã‚ã¦è²¼ã‚Šä»˜ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¤šã„ã®ã§
    # ã¾ãšã¯è¡Œã”ã¨ã®åŒºåˆ‡ã‚Šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åï¼‰ã§åˆ†å‰²ã—ã€ãã®ä¸­ã§ã•ã‚‰ã«ã€Œãƒ–ãƒ­ãƒƒã‚¯ã€ã‚’æ¢ã™
    
    df_full = df_full.astype(str)
    file_indices = df_full[df_full[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:", na=False)].index.tolist()
    
    file_chunks = []
    if not file_indices:
        file_chunks.append(df_full)
    else:
        for i in range(len(file_indices)):
            start_idx = file_indices[i]
            # ãƒ•ã‚¡ã‚¤ãƒ«åè¡Œè‡ªä½“ã¯ãƒ‡ãƒ¼ã‚¿ã§ã¯ãªã„ã®ã§ã‚¹ã‚­ãƒƒãƒ—ã—ãŸã„ãŒã€
            # ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦æ¸¡ã—ã¦å¾Œã§é™¤å¤–ã™ã‚‹
            end_idx = file_indices[i + 1] if i + 1 < len(file_indices) else len(df_full)
            file_chunks.append(df_full.iloc[start_idx:end_idx].reset_index(drop=True))

    all_extracted_dfs = []
    all_item_orders = []

    for chunk in file_chunks:
        # "ãƒ•ã‚¡ã‚¤ãƒ«å:" ã‚„ "--- ãƒšãƒ¼ã‚¸" ãªã©ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡Œã‚’é™¤å¤–ã—ã¦ç´”ç²‹ãªè¡¨ãƒ‡ãƒ¼ã‚¿ã«ã™ã‚‹
        clean_rows = []
        for idx, row in chunk.iterrows():
            row_txt = row.astype(str).str.cat()
            if "ãƒ•ã‚¡ã‚¤ãƒ«å:" in str(row[0]) or "--- ãƒšãƒ¼ã‚¸" in str(row[0]):
                continue
            clean_rows.append(row)
        
        if not clean_rows:
            continue
            
        df_clean = pd.DataFrame(clean_rows)
        
        # 2. ç©ºç™½åˆ—ã§ãƒ–ãƒ­ãƒƒã‚¯åˆ†å‰²
        blocks = split_into_horizontal_blocks(df_clean)
        
        # 3. å„ãƒ–ãƒ­ãƒƒã‚¯ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        for block in blocks:
            if block.empty: continue
            year, df_data = extract_data_from_block(block)
            
            if year and df_data is not None:
                all_extracted_dfs.append(df_data)
                all_item_orders.append(df_data["å…±é€šé …ç›®"].tolist())

    if not all_extracted_dfs:
        return None

    # 4. çµ±åˆãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒã‚¹ã‚¿é …ç›®ã®ä½œæˆãƒ»é †åºä¿æŒï¼‰
    master_items = []
    
    # å…¨ã¦ã®é …ç›®ãƒªã‚¹ãƒˆã‚’å·¡å›ã—ã¦ãƒã‚¹ã‚¿ãƒ¼ãƒªã‚¹ãƒˆã‚’è‚²ã¦ã‚‹
    for items in all_item_orders:
        if not master_items:
            master_items = list(items)
            continue
        
        # ç›¸å¯¾ä½ç½®ã‚’å­¦ç¿’ã—ãªãŒã‚‰æŒ¿å…¥
        last_known_idx = -1
        for item in items:
            if item in master_items:
                last_known_idx = master_items.index(item)
            else:
                # æœªçŸ¥ã®é …ç›®ï¼ˆBãªã©ï¼‰ã¯ã€ç›´å‰ã®æ—¢çŸ¥é …ç›®ã®å¾Œã‚ã«æŒ¿å…¥
                master_items.insert(last_known_idx + 1, item)
                last_known_idx += 1

    # 5. ãƒã‚¹ã‚¿ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä½œæˆã¨ãƒãƒ¼ã‚¸
    final_df = pd.DataFrame({"å…±é€šé …ç›®": master_items})
    
    for df in all_extracted_dfs:
        # å¹´æ¬¡ã‚«ãƒ©ãƒ åã‚’å–å¾—
        year_col = [c for c in df.columns if c != "å…±é€šé …ç›®"][0]
        
        # ãƒãƒ¼ã‚¸
        merged = pd.merge(final_df, df, on="å…±é€šé …ç›®", how="left")
        
        # æ—¢ã«åŒã˜å¹´ãŒã‚ã‚‹å ´åˆã¯ update (combine_first)
        if year_col in final_df.columns:
             final_df[year_col] = final_df[year_col].combine_first(merged[year_col])
        else:
             final_df[year_col] = merged[year_col]

    # 6. 0åŸ‹ã‚ã¨æ•´å½¢
    final_df = final_df.fillna(0)
    
    # å¹´æ¬¡é †ã«ä¸¦ã¹æ›¿ãˆï¼ˆé™é †ï¼‰
    cols = [c for c in final_df.columns if c != "å…±é€šé …ç›®"]
    cols.sort(key=lambda x: float(re.findall(r'\d+', str(x))[0]) if re.findall(r'\d+', str(x)) else 0, reverse=True)
    
    final_df = final_df[["å…±é€šé …ç›®"] + cols]
    
    # è¡¨ç¤ºç”¨ã«ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹å‰Šé™¤
    final_df["å…±é€šé …ç›®"] = final_df["å…±é€šé …ç›®"].str.replace(r"_temp_\d+$", "", regex=True)
    final_df["å…±é€šé …ç›®"] = final_df["å…±é€šé …ç›®"].str.replace(r"_\d+$", "", regex=True)

    return [final_df]


# ==========================================
# Streamlit UI
# ==========================================

st.set_page_config(page_title="å¤šæ©Ÿèƒ½ãƒ„ãƒ¼ãƒ«", layout="wide")
st.title("ğŸ“„ğŸ“Š å¤šæ©Ÿèƒ½ãƒ„ãƒ¼ãƒ«")

# --- ãƒ„ãƒ¼ãƒ«â‘  ---
with st.container(border=True):
    st.header("ãƒ„ãƒ¼ãƒ«â‘ ï¼šPDFè¡¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")
    pdf_files = st.file_uploader(
        "PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰", type="pdf", accept_multiple_files=True
    )
    keyword_input_str = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰")
    col1, col2 = st.columns(2)
    start_page_input = col1.text_input("é–‹å§‹ãƒšãƒ¼ã‚¸", placeholder="ä¾‹: 5")
    end_page_input = col2.text_input("çµ‚äº†ãƒšãƒ¼ã‚¸", placeholder="ä¾‹: 10")
    if st.button("æŠ½å‡ºé–‹å§‹ â–¶ï¸"):
        if pdf_files:
            keywords = [kw.strip() for kw in keyword_input_str.split(",") if kw.strip()]
            start_page = int(start_page_input) if start_page_input.isdigit() else None
            end_page = int(end_page_input) if end_page_input.isdigit() else None
            with st.spinner("PDFè§£æä¸­..."):
                df_result = extract_tables_from_multiple_pdfs(
                    pdf_files, keywords, start_page, end_page
                )
                if df_result is not None and not df_result.empty:
                    st.success("æŠ½å‡ºå®Œäº†ï¼", icon="âœ…")
                    st.dataframe(df_result)
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                        df_result.to_excel(writer, index=False, header=False, sheet_name="æŠ½å‡ºçµæœ")
                        workbook = writer.book
                        worksheet = writer.sheets["æŠ½å‡ºçµæœ"]
                        bold_format = workbook.add_format({"bold": True, "font_size": 20})
                        for idx, val in enumerate(df_result[0]):
                            if isinstance(val, str) and val.startswith("ãƒ•ã‚¡ã‚¤ãƒ«å:"):
                                worksheet.set_row(idx, None, bold_format)
                    
                    if keywords:
                        base_name = '_'.join(keywords)
                        download_filename = f"{base_name}_ã¾ã¨ã‚.xlsx"
                    else:
                        download_filename = "æŠ½å‡ºçµæœ_ã¾ã¨ã‚.xlsx"

                    st.download_button(
                        label="ğŸ“¥ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=output.getvalue(),
                        file_name=download_filename,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    )
        else:
            st.error("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", icon="ğŸš¨")

st.divider()

# --- ãƒ„ãƒ¼ãƒ«â‘¡ ---
with st.container(border=True):
    st.header("ãƒ„ãƒ¼ãƒ«â‘¡ï¼šçµ±åˆãƒ‡ãƒ¼ã‚¿ä½œæˆ")
    excel_file = st.file_uploader("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])
    
    # çµ±åˆæ–¹å‘ã®é¸æŠ
    st.subheader("çµ±åˆæ–¹å‘ã‚’é¸æŠ")
    direction = st.radio(
        "ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆæ–¹å‘ï¼š",
        options=["ç¸¦æ–¹å‘", "æ¨ªæ–¹å‘"],
        horizontal=True,
        help="ç¸¦æ–¹å‘ï¼šå¹´æ¬¡ãƒ‡ãƒ¼ã‚¿ãŒç¸¦ã«ä¸¦ã‚“ã§ã„ã‚‹å ´åˆ / æ¨ªæ–¹å‘ï¼šè¡¨ãŒæ¨ªã«è¤‡æ•°ä¸¦ã‚“ã§ã„ã‚‹å ´åˆ"
    )
    
    if st.button("çµ±åˆã¾ã¨ã‚è¡¨ã‚’ä½œæˆ â–¶ï¸", disabled=(excel_file is None)):
        with st.spinner("ãƒ‡ãƒ¼ã‚¿æ•´ç†ä¸­..."):
            if direction == "ç¸¦æ–¹å‘":
                all_summaries = process_files_and_tables_vertical(excel_file)
            else:  # æ¨ªæ–¹å‘
                all_summaries = process_files_and_tables_horizontal(excel_file)
            
            if all_summaries:
                st.success(f"âœ… {len(all_summaries)}å€‹ã®ã¾ã¨ã‚è¡¨ã‚’ä½œæˆï¼", icon="ğŸ‰")
                
                # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
                for i, summary_df in enumerate(all_summaries):
                    st.subheader(f"ğŸ“Š çµ±åˆã¾ã¨ã‚è¡¨_{i+1}")
                    st.dataframe(summary_df, use_container_width=True)
                
                output_excel = io.BytesIO()
                with pd.ExcelWriter(output_excel, engine="xlsxwriter") as writer:
                    for i, summary_df in enumerate(all_summaries):
                        summary_df.to_excel(
                            writer, sheet_name=f"çµ±åˆã¾ã¨ã‚è¡¨_{i+1}", index=False
                        )

                base_name_input = excel_file.name.rsplit('.xlsx', 1)[0]
                if base_name_input.endswith('_ã¾ã¨ã‚'):
                    base_name_output = base_name_input.removesuffix('_ã¾ã¨ã‚') + '_çµ±åˆ'
                else:
                    base_name_output = base_name_input + '_çµ±åˆ'
                download_filename = f"{base_name_output}.xlsx"

                st.download_button(
                    label="ğŸ“¥ çµ±åˆã¾ã¨ã‚è¡¨ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=output_excel.getvalue(),
                    file_name=download_filename,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
            else:
                st.warning("âš ï¸ çµ±åˆå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", icon="âš ï¸")
