import streamlit as st
import pandas as pd
import pdfplumber
import io
import re
from collections import defaultdict

# --- ãƒ„ãƒ¼ãƒ«â‘ ï¼šPDFã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°ï¼ˆè¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯¾å¿œï¼‰ ---
def extract_tables_from_multiple_pdfs(pdf_files, keywords, start_page, end_page):
    all_rows = []
    if not keywords:
        st.error("â— ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", icon="ğŸš¨")
        return None
    for pdf_file in pdf_files:
        all_rows.append([f"ãƒ•ã‚¡ã‚¤ãƒ«å: {pdf_file.name}"])
        all_rows.append([])
        found_in_file = False
        try:
            with pdfplumber.open(pdf_file) as pdf:
                start_index = start_page - 1 if start_page else 0
                end_index = end_page if end_page else len(pdf.pages)
                target_pages = pdf.pages[start_index:end_index]
                for page in target_pages:
                    text = page.extract_text() or ""
                    if any(kw in text for kw in keywords):
                        found_in_file = True
                        tables = page.extract_tables()
                        for table_index, table in enumerate(tables):
                            if not table:
                                continue
                            all_rows.append([f"--- ãƒšãƒ¼ã‚¸ {page.page_number} / ãƒ†ãƒ¼ãƒ–ãƒ« {table_index + 1} ---"])
                            for row in table:
                                cleaned_row = ["" if item is None else str(item).replace("\n", " ") for item in row]
                                all_rows.append(cleaned_row)
                            all_rows.append([])
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{pdf_file.name}ã€å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", icon="ğŸ”¥")
            continue
        if not found_in_file:
            st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{pdf_file.name}ã€ã§ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€è¡¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", icon="âš ï¸")

    if not any(r for r in all_rows if r):
        return None
    return pd.DataFrame(all_rows)


# --- ãƒ„ãƒ¼ãƒ«â‘¡ï¼šç¸¦æ–¹å‘çµ±åˆé–¢æ•° ---
def tool2_extract_data_from_chunk(df_chunk):
    if df_chunk.empty:
        return None, []
    
    quarter_pat = re.compile(r"^\s*(20\d{2}Q[1-4])\s*$", re.IGNORECASE)
    from_date_pat = re.compile(r"\(è‡ª\s*(\d{4})å¹´(\d{1,2})æœˆ")
    date_pat = re.compile(r"\((\d{4})å¹´(\d{1,2})æœˆ") 
    year_pat = re.compile(r"^\s*20\d{2}(\d{2})?\s*$")

    year_cells = []
    for r in range(df_chunk.shape[0]):
        for c in range(df_chunk.shape[1]):
            cell_value = str(df_chunk.iat[r, c]).strip()
            year_header = None

            match_q = quarter_pat.search(cell_value)
            match1 = from_date_pat.search(cell_value)
            match2 = date_pat.search(cell_value)

            if match_q:
                year_header = match_q.group(1).upper()
            elif match1:
                year = match1.group(1)
                month = match1.group(2)
                year_header = f"{year}/{month}"
            elif match2:
                year = match2.group(1)
                month = match2.group(2)
                year_header = f"{year}/{month}"
            elif cell_value.isdigit() and year_pat.match(cell_value):
                year_header = cell_value

            if year_header:
                year_cells.append({"row": r, "col": c, "year_header": year_header})

    if not year_cells:
        return None, []

    year_cells.sort(key=lambda x: (x["row"], x["col"]))
    processed_years = set()
    initial_items = df_chunk[0].astype(str).str.strip().dropna()
    initial_items = initial_items[initial_items != ""]
    is_sonota = initial_items == "ãã®ä»–"
    if is_sonota.any():
        sonota_counts = initial_items.groupby(initial_items).cumcount()
        initial_items.loc[is_sonota] = "ãã®ä»–_temp_" + sonota_counts[is_sonota].astype(str)
    all_items_ordered = initial_items.drop_duplicates(keep="first").tolist()
    df_result = pd.DataFrame({"å…±é€šé …ç›®": all_items_ordered})

    for cell in year_cells:
        year_header = cell["year_header"]
        if year_header in processed_years:
            continue
        processed_years.add(year_header)
        val_col = cell["col"]
        temp_df = df_chunk.iloc[cell["row"] + 1 :, [0, val_col]].copy()
        temp_df.columns = ["å…±é€šé …ç›®", year_header]
        temp_df["å…±é€šé …ç›®"] = temp_df["å…±é€šé …ç›®"].astype(str).str.strip()
        temp_df = temp_df[temp_df["å…±é€šé …ç›®"] != ""].dropna(subset=["å…±é€šé …ç›®"])
        is_sonota = temp_df["å…±é€šé …ç›®"] == "ãã®ä»–"
        if is_sonota.any():
            sonota_counts = temp_df.groupby("å…±é€šé …ç›®").cumcount()
            temp_df.loc[is_sonota, "å…±é€šé …ç›®"] = "ãã®ä»–_temp_" + sonota_counts[is_sonota].astype(str)
        temp_df[year_header] = (
            pd.to_numeric(temp_df[year_header].astype(str).str.replace(",", ""), errors='coerce').fillna(0)
        )
        temp_df = temp_df.drop_duplicates(subset=["å…±é€šé …ç›®"], keep="first")
        df_result = pd.merge(df_result, temp_df, on="å…±é€šé …ç›®", how="left")

    return df_result, all_items_ordered


# --- ãƒ„ãƒ¼ãƒ«â‘¡ï¼šæ¨ªæ–¹å‘çµ±åˆé–¢æ•°ï¼ˆå®Œå…¨ç‰ˆï¼‰ ---
def tool2_extract_data_horizontal(df_chunk):
    """æ¨ªæ–¹å‘ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã™ã‚‹é–¢æ•°
    
    æƒ³å®šã•ã‚Œã‚‹æ§‹é€ ï¼š
    - 1è¡Œç›®ã«å¹´æ¬¡ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ2024, 2023, 2022, 2021ãªã©ï¼‰
    - å„å¹´æ¬¡åˆ—ã®å·¦å´ã«é …ç›®åˆ—ï¼ˆå£²ä¸Šé«˜ã€å£²ä¸ŠåŸä¾¡ãªã©ï¼‰
    - ãƒ‡ãƒ¼ã‚¿ã¯2è¡Œç›®ä»¥é™
    """
    if df_chunk.empty:
        return None, []
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    st.write("ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’è§£æä¸­...")
    st.write(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df_chunk.shape[0]}è¡Œ Ã— {df_chunk.shape[1]}åˆ—")
    
    # 1è¡Œç›®ã‹ã‚‰å¹´æ¬¡ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¤œå‡ºï¼ˆ4æ¡ã®æ•°å­—ï¼‰
    year_columns = []
    first_row = df_chunk.iloc[0] if len(df_chunk) > 0 else pd.Series()
    
    for col_idx in range(len(first_row)):
        cell_value = str(first_row.iloc[col_idx]).strip()
        # 4æ¡ã®å¹´æ¬¡ã‚’æ¤œå‡ºï¼ˆ2000å¹´ä»£ï¼‰
        if cell_value.isdigit() and len(cell_value) == 4 and cell_value.startswith('20'):
            year_columns.append({
                "col_idx": col_idx,
                "year": cell_value
            })
            st.write(f"âœ“ å¹´æ¬¡åˆ—ã‚’æ¤œå‡º: {cell_value} (åˆ—{col_idx})")
    
    if not year_columns:
        st.warning("âš ï¸ å¹´æ¬¡ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆ4æ¡ã®è¥¿æš¦ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None, []
    
    st.write(f"ğŸ“… æ¤œå‡ºã•ã‚ŒãŸå¹´æ¬¡: {[y['year'] for y in year_columns]}")
    
    # å„å¹´æ¬¡åˆ—ã«ã¤ã„ã¦ã€å¯¾å¿œã™ã‚‹é …ç›®åˆ—ã¨ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    block_data = []
    
    for year_info in year_columns:
        year = year_info["year"]
        data_col = year_info["col_idx"]
        
        # é …ç›®åˆ—ã‚’æ¢ã™ï¼šãƒ‡ãƒ¼ã‚¿åˆ—ã®å·¦å´ã§æœ€ã‚‚è¿‘ã„ã€ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹åˆ—
        item_col = None
        
        # ãƒ‡ãƒ¼ã‚¿åˆ—ã®ç›´å‰ã®åˆ—ã‹ã‚‰å·¦ã«å‘ã‹ã£ã¦æ¢ç´¢
        for search_col in range(data_col - 1, -1, -1):
            # ãã®åˆ—ã®2è¡Œç›®ä»¥é™ã‚’ãƒã‚§ãƒƒã‚¯
            col_data = df_chunk.iloc[1:, search_col].astype(str).str.strip()
            
            # ç©ºã§ãªã„ã€ã‹ã¤æ„å‘³ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã‚‹ã‚»ãƒ«ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            valid_items = 0
            for val in col_data:
                if val and val != "nan" and val != "":
                    # æ•°å­—ã®ã¿ã€ã¾ãŸã¯ç©ºç™½ã®ã¿ã§ãªã„ã“ã¨ã‚’ç¢ºèª
                    clean_val = val.replace(",", "").replace(".", "").replace("-", "").replace(" ", "")
                    if not (clean_val.isdigit() or clean_val == ""):
                        valid_items += 1
            
            if valid_items >= 3:  # æœ€ä½3ã¤ä»¥ä¸Šã®æœ‰åŠ¹ãªé …ç›®
                item_col = search_col
                st.write(f"  â†’ {year}å¹´ã®é …ç›®åˆ—: åˆ—{item_col} (æœ‰åŠ¹é …ç›®æ•°: {valid_items})")
                break
        
        if item_col is None:
            st.warning(f"âš ï¸ {year}å¹´ã®é …ç›®åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            continue
        
        # é …ç›®ã¨æ•°å€¤ã‚’æŠ½å‡ºï¼ˆ1è¡Œç›®ã¯ãƒ˜ãƒƒãƒ€ãƒ¼ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        items = []
        values = []
        
        for row_idx in range(1, df_chunk.shape[0]):
            item = str(df_chunk.iloc[row_idx, item_col]).strip()
            value = str(df_chunk.iloc[row_idx, data_col]).strip()
            
            # é …ç›®ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿è¿½åŠ 
            if item and item != "nan" and item != "":
                items.append(item)
                values.append(value)
        
        if items:
            block_data.append({
                "year": year,
                "items": items,
                "values": values
            })
            st.write(f"  âœ“ {year}å¹´: {len(items)}é …ç›®ã‚’æŠ½å‡º")
    
    if not block_data:
        st.error("âŒ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãƒ–ãƒ­ãƒƒã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None, []
    
    st.success(f"âœ… {len(block_data)}å€‹ã®å¹´æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
    
    # å¹´æ¬¡ã§é™é †ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„å¹´ãŒå·¦ã«æ¥ã‚‹ã‚ˆã†ã«ï¼‰
    block_data.sort(key=lambda x: int(x["year"]), reverse=True)
    
    # å…¨é …ç›®ã®å’Œé›†åˆã‚’ä½œæˆï¼ˆæœ€åˆã®ãƒ–ãƒ­ãƒƒã‚¯ã®é †åºã‚’åŸºæº–ã«ã™ã‚‹ï¼‰
    all_items_ordered = []
    item_positions = {}  # é …ç›®ã®æœ€åˆã®å‡ºç¾ä½ç½®ã‚’è¨˜éŒ²
    
    # æœ€åˆã®ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆæœ€æ–°å¹´ï¼‰ã®é …ç›®é †åºã‚’åŸºæº–ã«ã™ã‚‹
    base_block = block_data[0]
    for idx, item in enumerate(base_block["items"]):
        if item not in item_positions:
            all_items_ordered.append(item)
            item_positions[item] = len(all_items_ordered) - 1
    
    # ä»–ã®ãƒ–ãƒ­ãƒƒã‚¯ã®é …ç›®ã§æ–°ã—ã„ã‚‚ã®ã‚’è¿½åŠ 
    for block in block_data[1:]:
        for item in block["items"]:
            if item not in item_positions:
                all_items_ordered.append(item)
                item_positions[item] = len(all_items_ordered) - 1
    
    st.write(f"ğŸ“‹ çµ±åˆå¾Œã®é …ç›®æ•°: {len(all_items_ordered)}")
    
    # ã€Œãã®ä»–ã€ã®é‡è¤‡ã‚’å‡¦ç†
    processed_items = []
    sonota_count = 0
    for item in all_items_ordered:
        if item == "ãã®ä»–":
            if sonota_count > 0:
                processed_items.append(f"ãã®ä»–_temp_{sonota_count}")
            else:
                processed_items.append(item)
            sonota_count += 1
        else:
            processed_items.append(item)
    
    # çµæœDataFrameã‚’æ§‹ç¯‰
    result_dict = {"å…±é€šé …ç›®": processed_items}
    
    for block in block_data:
        year = block["year"]
        year_values = []
        
        # é …ç›®â†’å€¤ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        item_value_map = {}
        sonota_values = []
        
        for i, item in enumerate(block["items"]):
            if item == "ãã®ä»–":
                sonota_values.append(block["values"][i])
            else:
                item_value_map[item] = block["values"][i]
        
        # å„é …ç›®ã«å¯¾å¿œã™ã‚‹å€¤ã‚’å–å¾—
        sonota_idx = 0
        for item in processed_items:
            value_str = None
            
            # ã€Œãã®ä»–_temp_Nã€ã®å‡¦ç†
            if item.startswith("ãã®ä»–"):
                if sonota_idx < len(sonota_values):
                    value_str = sonota_values[sonota_idx]
                    sonota_idx += 1
            else:
                value_str = item_value_map.get(item)
            
            # æ•°å€¤å¤‰æ›
            if value_str:
                clean_value = str(value_str).replace(",", "").strip()
                try:
                    if clean_value and clean_value != "nan":
                        value = float(clean_value)
                        # æ•´æ•°ã«å¤‰æ›ã§ãã‚‹å ´åˆã¯æ•´æ•°ã«
                        if value == int(value):
                            value = int(value)
                    else:
                        value = 0
                except:
                    value = 0
            else:
                value = 0
            
            year_values.append(value)
        
        result_dict[year] = year_values
    
    result_df = pd.DataFrame(result_dict)
    
    # ã€Œãã®ä»–_temp_ã€ã‚’ã€Œãã®ä»–ã€ã«æˆ»ã™
    result_df["å…±é€šé …ç›®"] = result_df["å…±é€šé …ç›®"].str.replace(r"_temp_\d+$", "", regex=True)
    
    # é …ç›®ã®é †åºã‚’ä¿å­˜
    item_order = result_df["å…±é€šé …ç›®"].tolist()
    
    st.write("ğŸ‰ çµ±åˆå®Œäº†ï¼")
    
    return result_df, item_order


def process_files_and_tables_vertical(excel_file):
    """ç¸¦æ–¹å‘ã®çµ±åˆå‡¦ç†ï¼ˆå…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
    try:
        xls = pd.ExcelFile(excel_file)
        sheet_name_to_read = "æŠ½å‡ºçµæœ" if "æŠ½å‡ºçµæœ" in xls.sheet_names else xls.sheet_names[0]
        df_full = pd.read_excel(xls, sheet_name=sheet_name_to_read, header=None)
    except Exception as e:
        st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return None

    df_full[0] = df_full[0].astype(str)
    file_indices = df_full[df_full[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:", na=False)].index.tolist()
    file_chunks = []
    if not file_indices:
        file_chunks.append(df_full)
    else:
        for i in range(len(file_indices)):
            start_idx = file_indices[i]
            end_idx = file_indices[i + 1] if i + 1 < len(file_indices) else len(df_full)
            file_chunks.append(df_full.iloc[start_idx:end_idx].reset_index(drop=True))

    grouped_tables = defaultdict(list)
    master_item_order = defaultdict(list)

    for file_chunk in file_chunks:
        page_indices = file_chunk[file_chunk[0].str.contains(r"--- ãƒšãƒ¼ã‚¸", na=False)].index.tolist()
        table_chunks = []
        last_idx = 0
        if not page_indices:
            clean_chunk = file_chunk[
                ~file_chunk[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:|---|^\s*$", na=False, regex=True)
            ].dropna(how="all")
            if not clean_chunk.empty:
                table_chunks.append(clean_chunk)
        else:
            for idx in page_indices:
                chunk = file_chunk.iloc[last_idx:idx]
                if not chunk.empty:
                    table_chunks.append(chunk)
                last_idx = idx
            final_chunk = file_chunk.iloc[last_idx:]
            if not final_chunk.empty:
                table_chunks.append(final_chunk)

        for i, table_chunk in enumerate(table_chunks):
            clean_table_chunk = table_chunk[
                ~table_chunk[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:|---", na=False, regex=True)
            ].dropna(how="all")
            if clean_table_chunk.empty:
                continue
            processed_df, item_order = tool2_extract_data_from_chunk(
                clean_table_chunk.reset_index(drop=True)
            )
            if processed_df is not None and not processed_df.empty:
                grouped_tables[i].append(processed_df)
                current_master_order = master_item_order[i]
                if not current_master_order:
                    master_item_order[i].extend(item_order)
                else:
                    last_known_index = -1
                    for item in item_order:
                        if item in current_master_order:
                            last_known_index = current_master_order.index(item)
                        else:
                            current_master_order.insert(last_known_index + 1, item)
                            last_known_index += 1

    final_summaries = []
    for table_index in sorted(grouped_tables.keys()):
        list_of_dfs = grouped_tables[table_index]
        ordered_items = master_item_order[table_index]
        if not list_of_dfs:
            continue
        result_df = pd.DataFrame({"å…±é€šé …ç›®": ordered_items})
        for df_to_merge in list_of_dfs:
            cols_to_drop = [
                col for col in df_to_merge.columns if col in result_df.columns and col != "å…±é€šé …ç›®"
            ]
            result_df = pd.merge(
                result_df, df_to_merge.drop(columns=cols_to_drop), on="å…±é€šé …ç›®", how="left"
            )
        result_df.fillna(0, inplace=True)
        year_cols = sorted(
            [col for col in result_df.columns if col != "å…±é€šé …ç›®"],
            key=lambda x: int(str(x).upper().replace('/', '').replace('Q', '0').ljust(6, '0'))
        )
        final_cols = ["å…±é€šé …ç›®"] + year_cols
        result_df = result_df[final_cols]
        for col in year_cols:
            result_df[col] = pd.to_numeric(result_df[col], errors='coerce').fillna(0).astype(int)
        result_df["å…±é€šé …ç›®"] = result_df["å…±é€šé …ç›®"].str.replace(r"_temp_\d+$", "", regex=True)
        final_summaries.append(result_df)
    return final_summaries


def process_files_and_tables_horizontal(excel_file):
    """æ¨ªæ–¹å‘ã®çµ±åˆå‡¦ç†"""
    try:
        xls = pd.ExcelFile(excel_file)
        sheet_name_to_read = "æŠ½å‡ºçµæœ" if "æŠ½å‡ºçµæœ" in xls.sheet_names else xls.sheet_names[0]
        df_full = pd.read_excel(xls, sheet_name=sheet_name_to_read, header=None)
    except Exception as e:
        st.error(f"Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        return None

    st.write(f"ğŸ“– èª­ã¿è¾¼ã‚“ã ã‚·ãƒ¼ãƒˆ: {sheet_name_to_read}")
    st.write(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df_full.shape[0]}è¡Œ Ã— {df_full.shape[1]}åˆ—")
    
    df_full = df_full.astype(str)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã§ãƒãƒ£ãƒ³ã‚¯ã‚’åˆ†å‰²
    file_indices = df_full[df_full[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:", na=False)].index.tolist()
    file_chunks = []
    if not file_indices:
        file_chunks.append(df_full)
    else:
        for i in range(len(file_indices)):
            start_idx = file_indices[i]
            end_idx = file_indices[i + 1] if i + 1 < len(file_indices) else len(df_full)
            file_chunks.append(df_full.iloc[start_idx:end_idx].reset_index(drop=True))
    
    st.write(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ£ãƒ³ã‚¯æ•°: {len(file_chunks)}")

    all_table_results = []
    chunk_counter = 0

    for file_idx, file_chunk in enumerate(file_chunks):
        st.write(f"\n--- ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ£ãƒ³ã‚¯ {file_idx + 1} ã‚’å‡¦ç†ä¸­ ---")
        
        # ãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Šã‚’æ¤œå‡º
        page_indices = file_chunk[file_chunk[0].str.contains(r"--- ãƒšãƒ¼ã‚¸", na=False)].index.tolist()
        table_chunks = []
        
        if not page_indices:
            clean_chunk = file_chunk[
                ~file_chunk[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:|---|^\s*$", na=False, regex=True)
            ].dropna(how="all")
            if not clean_chunk.empty:
                table_chunks.append(clean_chunk)
        else:
            last_idx = 0
            for idx in page_indices:
                chunk = file_chunk.iloc[last_idx:idx]
                clean_chunk = chunk[
                    ~chunk[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:|---", na=False, regex=True)
                ].dropna(how="all")
                if not clean_chunk.empty:
                    table_chunks.append(clean_chunk)
                last_idx = idx
            
            final_chunk = file_chunk.iloc[last_idx:]
            clean_chunk = final_chunk[
                ~final_chunk[0].str.contains(r"ãƒ•ã‚¡ã‚¤ãƒ«å:|---", na=False, regex=True)
            ].dropna(how="all")
            if not clean_chunk.empty:
                table_chunks.append(clean_chunk)
        
        st.write(f"  ãƒ†ãƒ¼ãƒ–ãƒ«ãƒãƒ£ãƒ³ã‚¯æ•°: {len(table_chunks)}")

        # å„ãƒ†ãƒ¼ãƒ–ãƒ«ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
        for table_idx, table_chunk in enumerate(table_chunks):
            if table_chunk.empty:
                continue
            
            chunk_counter += 1
            st.write(f"\nğŸ” ãƒ†ãƒ¼ãƒ–ãƒ« {chunk_counter} ã‚’è§£æä¸­...")
            
            processed_df, item_order = tool2_extract_data_horizontal(
                table_chunk.reset_index(drop=True)
            )
            
            if processed_df is not None and not processed_df.empty:
                all_table_results.append(processed_df)
                st.write(f"âœ… ãƒ†ãƒ¼ãƒ–ãƒ« {chunk_counter}: çµ±åˆæˆåŠŸ")
            else:
                st.write(f"âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ« {chunk_counter}: ãƒ‡ãƒ¼ã‚¿ãªã—")

    if not all_table_results:
        st.error("âŒ çµ±åˆå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return None

    st.success(f"ğŸŠ åˆè¨ˆ {len(all_table_results)} å€‹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’çµ±åˆã—ã¾ã—ãŸ")
    return all_table_results


# --- Streamlit UI ---
st.set_page_config(page_title="å¤šæ©Ÿèƒ½ãƒ„ãƒ¼ãƒ«", layout="wide")
st.title("ğŸ“„ğŸ“Š å¤šæ©Ÿèƒ½ãƒ„ãƒ¼ãƒ«")

# --- ãƒ„ãƒ¼ãƒ«â‘  ---
with st.container(border=True):
    st.header("ãƒ„ãƒ¼ãƒ«â‘ ï¼šPDFè¡¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡º")
    pdf_files = st.file_uploader(
        "PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°å¯ï¼‰", type="pdf", accept_multiple_files=True
    )
    keyword_input_str = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰")
    col1, col2 = st.columns(2)
    start_page_input = col1.text_input("é–‹å§‹ãƒšãƒ¼ã‚¸", placeholder="ä¾‹: 5")
    end_page_input = col2.text_input("çµ‚äº†ãƒšãƒ¼ã‚¸", placeholder="ä¾‹: 10")
    if st.button("æŠ½å‡ºé–‹å§‹ â–¶ï¸"):
        if pdf_files:
            keywords = [kw.strip() for kw in keyword_input_str.split(",") if kw.strip()]
            start_page = int(start_page_input) if start_page_input.isdigit() else None
            end_page = int(end_page_input) if end_page_input.isdigit() else None
            with st.spinner("PDFè§£æä¸­..."):
                df_result = extract_tables_from_multiple_pdfs(
                    pdf_files, keywords, start_page, end_page
                )
                if df_result is not None and not df_result.empty:
                    st.success("æŠ½å‡ºå®Œäº†ï¼", icon="âœ…")
                    st.dataframe(df_result)
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                        df_result.to_excel(writer, index=False, header=False, sheet_name="æŠ½å‡ºçµæœ")
                        workbook = writer.book
                        worksheet = writer.sheets["æŠ½å‡ºçµæœ"]
                        bold_format = workbook.add_format({"bold": True, "font_size": 20})
                        for idx, val in enumerate(df_result[0]):
                            if isinstance(val, str) and val.startswith("ãƒ•ã‚¡ã‚¤ãƒ«å:"):
                                worksheet.set_row(idx, None, bold_format)
                    
                    if keywords:
                        base_name = '_'.join(keywords)
                        download_filename = f"{base_name}_ã¾ã¨ã‚.xlsx"
                    else:
                        download_filename = "æŠ½å‡ºçµæœ_ã¾ã¨ã‚.xlsx"

                    st.download_button(
                        label="ğŸ“¥ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=output.getvalue(),
                        file_name=download_filename,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    )
        else:
            st.error("PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", icon="ğŸš¨")

st.divider()

# --- ãƒ„ãƒ¼ãƒ«â‘¡ ---
with st.container(border=True):
    st.header("ãƒ„ãƒ¼ãƒ«â‘¡ï¼šçµ±åˆãƒ‡ãƒ¼ã‚¿ä½œæˆ")
    excel_file = st.file_uploader("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])
    
    # çµ±åˆæ–¹å‘ã®é¸æŠï¼ˆãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ï¼‰
    st.subheader("çµ±åˆæ–¹å‘ã‚’é¸æŠ")
    direction = st.radio(
        "ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆæ–¹å‘ï¼š",
        options=["ç¸¦æ–¹å‘", "æ¨ªæ–¹å‘"],
        horizontal=True,
        help="ç¸¦æ–¹å‘ï¼šå¹´æ¬¡ãƒ‡ãƒ¼ã‚¿ãŒç¸¦ã«ä¸¦ã‚“ã§ã„ã‚‹å ´åˆ / æ¨ªæ–¹å‘ï¼šè¡¨ãŒæ¨ªã«è¤‡æ•°ä¸¦ã‚“ã§ã„ã‚‹å ´åˆï¼ˆ1è¡Œç›®ã«å¹´æ¬¡ã€å„ãƒ–ãƒ­ãƒƒã‚¯ã«é …ç›®åˆ—ã¨æ•°å€¤åˆ—ï¼‰"
    )
    
    if st.button("çµ±åˆã¾ã¨ã‚è¡¨ã‚’ä½œæˆ â–¶ï¸", disabled=(excel_file is None)):
        with st.spinner("ãƒ‡ãƒ¼ã‚¿æ•´ç†ä¸­..."):
            if direction == "ç¸¦æ–¹å‘":
                all_summaries = process_files_and_tables_vertical(excel_file)
            else:  # æ¨ªæ–¹å‘
                all_summaries = process_files_and_tables_horizontal(excel_file)
            
            if all_summaries:
                st.success(f"âœ… {len(all_summaries)}å€‹ã®ã¾ã¨ã‚è¡¨ã‚’ä½œæˆï¼", icon="ğŸ‰")
                
                # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
                for i, summary_df in enumerate(all_summaries):
                    st.subheader(f"ğŸ“Š çµ±åˆã¾ã¨ã‚è¡¨_{i+1}")
                    st.dataframe(summary_df, use_container_width=True)
                
                output_excel = io.BytesIO()
                with pd.ExcelWriter(output_excel, engine="xlsxwriter") as writer:
                    for i, summary_df in enumerate(all_summaries):
                        summary_df.to_excel(
                            writer, sheet_name=f"çµ±åˆã¾ã¨ã‚è¡¨_{i+1}", index=False
                        )

                base_name_input = excel_file.name.rsplit('.xlsx', 1)[0]
                if base_name_input.endswith('_ã¾ã¨ã‚'):
                    base_name_output = base_name_input.removesuffix('_ã¾ã¨ã‚') + '_çµ±åˆ'
                else:
                    base_name_output = base_name_input + '_çµ±åˆ'
                download_filename = f"{base_name_output}.xlsx"

                st.download_button(
                    label="ğŸ“¥ çµ±åˆã¾ã¨ã‚è¡¨ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=output_excel.getvalue(),
                    file_name=download_filename,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
            else:
                st.warning("âš ï¸ çµ±åˆå¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", icon="âš ï¸")
